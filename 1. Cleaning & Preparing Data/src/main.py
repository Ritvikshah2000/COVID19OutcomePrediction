# -*- coding: utf-8 -*-
"""Milestone1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1cKGYfETgpqmWCXuHgw2Lab9gwAslduWT

#Import Libraries
# """
# !pip install pandas
# !pip install numpy
# pip install seaborn


import pandas as pd
import numpy as np
import math
import seaborn as sns
from scipy import stats

"""#Import data"""

casesTrain = pd.read_csv('https://raw.githubusercontent.com/MadanKrishnan97/CMPT459CourseProjectSpring2021/main/dataset/cases_train.csv')
casesTest = pd.read_csv('https://raw.githubusercontent.com/MadanKrishnan97/CMPT459CourseProjectSpring2021/main/dataset/cases_test.csv')
location = pd.read_csv('https://raw.githubusercontent.com/MadanKrishnan97/CMPT459CourseProjectSpring2021/main/dataset/location.csv')
population = pd.read_csv('https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/UID_ISO_FIPS_LookUp_Table.csv')

"""#1.1 Exploratory Data Analysis"""

def cleanAge(age):
    if type(age) == float:
        if age < 1:
            return 1
        return int(math.ceil(float(age)))
    if type(age) != int and '+' in age:
        split_age = age.split("+")
        return int(split_age[0])
    if type(age) != int and 'month' in age:
        return 1
    if type(age) != int and 'May' in age:
        return 1
    if type(age) != int and 'Oct' in age:
        return 1
    if type(age) != int and '-' in age:
        split_age = age.split("-")
        lower_age = int(split_age[0])
        if split_age[1] != '':
            upper_age = int(split_age[1])
        else:
            upper_age = 0
        avg_age = math.floor((lower_age + upper_age)/2)
        return int(avg_age)
    return int(math.ceil(float(age)))
    
def getAverageAge(ages):
    count = 0
    total = 0
    for age in ages:
        if age != -1 and type(age) == int:
            count = count + 1
            total += age
    return math.floor(total/count)  

def cleanCases(data):
    data = data.drop(['source', 'additional_information'], axis=1) ## drop irrelevant columns
    data.age.fillna(-1, inplace=True) ## replace all Nan with -1
    data.sex.fillna('unknown', inplace=True) ## replace all Nan with -1
    data['age'] = data.age.apply(cleanAge)  ## format all ages. If age is a range get the middle of range, if range is float convert to int. All months and fractional ages are bumped to age 1
    avg = getAverageAge(data['age'])
    data['age'] = data['age'].replace({-1:avg})   ## replace Nan(-1) with average age rounded down
    data['date_confirmation'] = data['date_confirmation'].replace({'\.':'/'}, regex=True)
    data['date_confirmation'] = data['date_confirmation'].str.slice(0,10)
    data['date_confirmation'] = pd.to_datetime(data['date_confirmation'])
    return data

def addCombineKey(data, countryInd, provinceInd):
    data['combinedKey'] = ''
    for ind in data.index:
      country = data.iloc[ind,countryInd]
      province = data.iloc[ind,provinceInd]
      if pd.isna(data.iloc[ind,provinceInd]) == True:
        data.at[ind,"combinedKey"] = country
      else:
        joined = str(province) + ", " + str(country)
        data.at[ind,"combinedKey"] = joined 
    return data

casesTest.loc[casesTest['country'] == 'United States', 'country'] = 'US'
casesTrain.loc[casesTrain['country'] == 'United States', 'country'] = 'US'

casesTest = cleanCases(casesTest)
casesTrain = cleanCases(casesTrain)

addCombineKey(casesTest, 3, 2)
addCombineKey(casesTrain, 3, 2)

#casesTrain = casesTrain.drop('province', 1)
#casesTrain = casesTrain.drop('country', 1)
casesTrain = casesTrain.drop('latitude', 1)
casesTrain = casesTrain.drop('longitude', 1)

#casesTest = casesTest.drop('province', 1)
#casesTest = casesTest.drop('country', 1)
casesTest = casesTest.drop('latitude', 1)
casesTest = casesTest.drop('longitude', 1)

casesTrain = casesTrain[pd.notnull(casesTrain['date_confirmation'])]
casesTrain = casesTrain[pd.notnull(casesTrain['combinedKey'])]
casesTest = casesTrain[pd.notnull(casesTrain['date_confirmation'])]
casesTest = casesTrain[pd.notnull(casesTrain['combinedKey'])]

tempcasesTrain = casesTrain
tempcasesTest = casesTest
tempcasesTest = tempcasesTest.drop('province', 1)
tempcasesTest = tempcasesTest.drop('country', 1)
tempcasesTrain = tempcasesTrain.drop('province', 1)
tempcasesTrain = tempcasesTrain.drop('country', 1)

tempcasesTrain.to_csv('results/cases_train_processed.csv', index = False, header = True)
tempcasesTest.to_csv('results/cases_test_processed.csv', index = False, header = True)


casesTrain.isna().sum()
casesTest.isna().sum()

casesTrain

"""
```
Cleaning Location
```


"""

location = location.drop(['Last_Update'], axis=1) ## only 3 different dates, not meaningful

location.Active.fillna(0, inplace=True)
location.Incidence_Rate.fillna(0, inplace=True)  ## most Nan incidence Rate and case-fatality ratio are dividing by zero or zero devided by denominator and hence no valid value or zero
location['Case-Fatality_Ratio'].fillna(0, inplace=True)
location.Active = location.Active.apply(lambda x: int(x))
location['Active'] = location['Active'].clip(lower=0)  ## negative should be 0

location.head()

location.isna().sum()
location[location['Incidence_Rate'].isnull()]

location.describe()

location['Confirmed'].replace(0,1,inplace=True)
location['Deaths'].replace(0,1,inplace=True)
location['Recovered'].replace(0,1,inplace=True)
location['Active'].replace(0,1,inplace=True)
location['Incidence_Rate'].replace(0,1,inplace=True)
location['Case-Fatality_Ratio'].replace(0,1,inplace=True)

# location['Confirmed'] = np.log10(location['Confirmed'])
# location['Deaths'] = np.log10(location['Deaths'])
# location['Recovered'] = np.log10(location['Recovered'])
# location['Active'] = np.log10(location['Active'])
# location['Incidence_Rate'] = np.log10(location['Incidence_Rate'])
# location['Case-Fatality_Ratio'] = np.log10(location['Case-Fatality_Ratio'])
location.describe()

"""#1.3 Dealing with outliers"""

location_o = location.copy()
location_o.rename(columns={'Case-Fatality_Ratio': 'Case_Fatality_Ratio'}, inplace=True)

threshold = 3

Confirmed_z = np.abs(stats.zscore(location_o.Confirmed))
location_o['Confirmed_zscore'] = Confirmed_z
location_o = location_o[(Confirmed_z<3)]

Deaths_z = np.abs(stats.zscore(location_o.Deaths))
location_o['Deaths_zscore'] = Deaths_z
location_o = location_o[(Deaths_z<3)]

Recovered_z = np.abs(stats.zscore(location_o.Recovered))
location_o['Recovered_zscore'] = Recovered_z
location_o = location_o[(Recovered_z<3)]

Active_z = np.abs(stats.zscore(location_o.Active))
location_o['Active_zscore'] = Active_z
location_o = location_o[(Active_z<3)]

Incidence_Rate_z = np.abs(stats.zscore(location_o.Incidence_Rate))
location_o['Incidence_Rate_zscore'] = Incidence_Rate_z
location_o = location_o[(Incidence_Rate_z<3)]

Case_Fatality_Ratio_z = np.abs(stats.zscore(location_o.Case_Fatality_Ratio))
location_o['Case_Fatality_Ratio_zscore'] = Case_Fatality_Ratio_z
location_o = location_o[(Case_Fatality_Ratio_z<3)]

sns.set_theme(style="whitegrid")
boxplotConfirmed = sns.boxplot(x=location["Confirmed"])
boxplotConfirmed.figure.savefig("plots/boxplotConfirmed.png")

sns.set_theme(style="whitegrid")
boxplotDeaths = sns.boxplot(x=location["Deaths"])
boxplotDeaths.figure.savefig("plots/boxplotDeaths.png")

sns.set_theme(style="whitegrid")
boxplotRecovered = sns.boxplot(x=location["Recovered"])
boxplotRecovered.figure.savefig("plots/boxplotRecovered.png")

sns.set_theme(style="whitegrid")
boxplotActive = sns.boxplot(x=location["Active"])
boxplotActive.figure.savefig("plots/boxplotActive.png")

sns.set_theme(style="whitegrid")
boxplotIncidence = sns.boxplot(x=location["Incidence_Rate"])
boxplotIncidence.figure.savefig("plots/boxplotIncidence.png")

sns.set_theme(style="whitegrid")
boxplotFatality = sns.boxplot(x=location["Case-Fatality_Ratio"])
boxplotFatality.figure.savefig("plots/boxplotFatality.png")

location_log = location.copy()
location_log['Confirmed_log'] = np.log10(location['Confirmed'])
location_log['Deaths_log'] = np.log10(location['Deaths'])
location_log['Recovered_log'] = np.log10(location['Recovered'])
location_log['Active_log'] = np.log10(location['Active'])
location_log['Incidence_Rate_log'] = np.log10(location['Incidence_Rate'])
location_log['Case-Fatality_Ratio_log'] = np.log10(location['Case-Fatality_Ratio'])

sns.set_theme(style="whitegrid")
boxplotConfirmedLog = sns.boxplot(x=location_log["Confirmed_log"])
boxplotConfirmedLog.figure.savefig("plots/boxplotConfirmedLog.png")

sns.set_theme(style="whitegrid")
boxplotDeathsLog = sns.boxplot(x=location_log["Deaths_log"])
boxplotDeathsLog.figure.savefig("plots/boxplotDeathsLog.png")

sns.set_theme(style="whitegrid")
boxplotRecoveredLog = sns.boxplot(x=location_log["Recovered_log"])
boxplotRecoveredLog.figure.savefig("plots/boxplotRecoveredLog.png")

sns.set_theme(style="whitegrid")
boxplotActiveLog = sns.boxplot(x=location_log["Active_log"])
boxplotActiveLog.figure.savefig("plots/boxplotActiveLog.png")

sns.set_theme(style="whitegrid")
boxplotIncidenceLog = sns.boxplot(x=location_log["Incidence_Rate_log"])
boxplotIncidenceLog.figure.savefig("plots/boxplotIncidenceLog.png")

sns.set_theme(style="whitegrid")
boxplotFatalityLog = sns.boxplot(x=location_log["Case-Fatality_Ratio_log"])
boxplotFatalityLog.figure.savefig("plots/boxplotFatalityLog.png")

"""#1.4 Transformation"""

US = location[location["Country_Region"] == "US"]
states = pd.DataFrame(US["Province_State"].unique()).values
locationTransformed = pd.DataFrame(columns = location.columns)
for state in states:
  latitude = longitude = confirmed = deaths = recovered = active = incidenceRate = temp = 0
  for i in range(US.shape[0]):
    temp += 1
    province = US.iloc[i]
    if province["Province_State"] == state:
      confirmed += province["Confirmed"]
      deaths += province["Deaths"]
      recovered += province["Recovered"]
      active += province["Active"]
      incidenceRate += province["Incidence_Rate"]
      if not math.isnan(province["Lat"]):
        latitude = province["Lat"]
      if not math.isnan(province["Long_"]):
        longitude = province["Long_"]

  locationTransformed = locationTransformed.append(pd.DataFrame({"Province_State": state, "Country_Region": "US", "Last_Update": "WIP", "Lat": latitude, "Long_": longitude, "Confirmed": confirmed, "Deaths": deaths, "Recovered": recovered, "Active": active, "Combined_Key": (state + ", US")}), ignore_index = True)


locationTransformed

merged = pd.merge(locationTransformed, population, left_on="Combined_Key", right_on="Combined_Key")

merged = merged.drop(['Last_Update', 'UID', 'iso2', 'iso3', 'code3', 'FIPS', 'Admin2', 'Province_State_y', 'Country_Region_y', 'Lat_y', 'Long__y'], axis = 1)
merged = merged[merged['Province_State_x'] != 'Recovered'] 
merged['Incidence_Rate'] = merged['Confirmed'] / merged['Population'] * 100000
merged['Case-Fatality_Ratio'] = merged['Deaths'] / merged['Confirmed'] * 100
merged

merged.columns = ['Province', 'Country', 'Latitude', 'Longitude', 'Confirmed', 'Deaths', 'Recovered', 'Active', 'combinedKey', 'incidenceRate', 'cases-fatality-ratio', 'Population']
merged

location = location[location['Country_Region'] != 'US']

location.append(merged)

location = location.rename(columns={'Long_': 'Long', 'Case-Fatality_Ratio': 'Case_Fatality_Ratio'})
location.to_csv('results/location_transformed.csv', index = False, header = True)

"""#1.5 Joining the cases and location dataset

1. The training dataset and the location will be joined by first making a combined key for the location dataset and then performing a left join. We cannot use longitude/latitude values as the training dataset and location dataset seem to have slightly different values for the same location.

2. We perform a left join on the datasets so that we ensure that we retain all the records in the training set, and simply use the location dataset to make the data more detailed. Moreover, performing a left join will ensure that only the rows that have the same combinedkey as rows in training dataset will be added, ensuring that there is no redundant information.

3. We drop the repeated lat/long columns & both sets of province and country as they are redundant. If one finds lat/long hard to interpret, they can refer to Combined_key which provices province and country.

4. By joining the two dataframes, we receive several important columns of information, each correctly with their location.
"""

casesTrain.head()

location.head()

missingLocationProvince = location['Province_State'].isna().sum()
missingLocationCountry = location['Country_Region'].isna().sum()
missingLocationLat = location['Lat'].isna().sum()
missingLocationLong = location['Long'].isna().sum()

locationMerged = pd.merge(casesTrain, location,  how='left', left_on=['combinedKey'], right_on = ['Combined_Key'])
locationMerged

locationMerged.drop(['Province_State', 'Country_Region', 'Lat', 'Long', 'Combined_Key'], axis = 1)
missingLocationMerged = locationMerged.isna().sum()

#merging test and location:
locationTestMerged = pd.merge(casesTest, location,  how='left', left_on=['combinedKey'], right_on = ['Combined_Key'])
locationTestMerged

locationTestMerged.drop(['Province_State', 'Country_Region', 'Lat', 'Long', 'Combined_Key'], axis = 1)