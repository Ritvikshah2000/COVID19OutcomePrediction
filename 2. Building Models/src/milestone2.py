# -*- coding: utf-8 -*-
"""Milestone2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_1XZrKsiLsKzvDV4fI4hVeoyp1Ksa61I
"""

!pip install graphviz

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import math
import seaborn as sns
from scipy import stats
from sklearn.model_selection import train_test_split
import xgboost as xgb
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import StratifiedKFold
from sklearn.preprocessing import OrdinalEncoder
from sklearn.metrics import accuracy_score
from sklearn.metrics import classification_report
import pickle
from sklearn.metrics import confusion_matrix
# %matplotlib inline
from matplotlib import pyplot as plt
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn import tree
from matplotlib import pyplot
from sklearn.model_selection import StratifiedKFold
from sklearn.model_selection import cross_val_score
from sklearn.metrics import roc_curve, auc,recall_score,precision_score

dataset = pd.read_csv('https://raw.githubusercontent.com/jusung1459/data/main/data.txt')
dataset['date_confirmation'] = pd.to_datetime(dataset['date_confirmation'])

dataset.shape

dataset.dtypes

dataset

"""# 2.1 Splitting Dataset"""

train, validation = train_test_split(dataset, test_size = 0.20, random_state = 42)

x_train = train[['age', 'sex', 'date_confirmation', 'combinedKey', 'Confirmed', 'Deaths', 'Recovered', 'Active', 'Incidence_Rate', 'Case-Fatality_Ratio']]
y_train = train[['outcome']]

x_validation = validation[['age', 'sex', 'date_confirmation', 'combinedKey', 'Confirmed', 'Deaths', 'Recovered', 'Active', 'Incidence_Rate', 'Case-Fatality_Ratio']]
y_validation = validation[['outcome']]

# encoder (categorical to numerical)
ord_enc = OrdinalEncoder()
encoded_dataSet = dataset.copy()
encoded_dataSet["sex"] = ord_enc.fit_transform(encoded_dataSet[["sex"]])
encoded_dataSet["outcome"] = ord_enc.fit_transform(encoded_dataSet[["outcome"]])
encoded_dataSet["combinedKey"] = ord_enc.fit_transform(encoded_dataSet[["combinedKey"]])
encoded_dataSet["date_confirmation"] = ord_enc.fit_transform(encoded_dataSet[["date_confirmation"]])
train_encoded, validation_encoded = train_test_split(encoded_dataSet, test_size = 0.20, random_state = 42)
x_train_encoded = train_encoded[['age', 'sex', 'date_confirmation', 'combinedKey', 'Confirmed', 'Deaths', 'Recovered', 'Active', 'Incidence_Rate', 'Case-Fatality_Ratio']]
y_train_encoded = train_encoded[['outcome']]

x_validation_encoded = validation_encoded[['age', 'sex', 'date_confirmation', 'combinedKey', 'Confirmed', 'Deaths', 'Recovered', 'Active', 'Incidence_Rate', 'Case-Fatality_Ratio']]
y_validation_encoded = validation_encoded[['outcome']]

encoded_dataSet

"""# 2.2"""

## finding hyperperameters learning rate
xgb_model = xgb.XGBClassifier(objective="multi:softprob", random_state=42, eval_metric="merror")
# xgb_model.fit(x_train_encoded, y_train_encoded)
n_estimators = [100, 200, 300, 400, 500]
learning_rate = [0.001, 0.01, 0.1, 0.2, 0.3]
param_grid = dict(learning_rate=learning_rate, n_estimators=n_estimators)
kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)
grid_search = GridSearchCV(xgb_model, param_grid, scoring="neg_log_loss", n_jobs=-1, cv=kfold)
grid_result = grid_search.fit(x_train_encoded, y_train_encoded)
print("Best: %f using %s" % (grid_result.best_score_, grid_result.best_params_))
means = grid_result.cv_results_['mean_test_score']  
stds = grid_result.cv_results_['std_test_score']
params = grid_result.cv_results_['params']
for mean, stdev, param in zip(means, stds, params):
	print("%f (%f) with: %r" % (mean, stdev, param))

scores = np.array(means).reshape(len(learning_rate), len(n_estimators))
for i, value in enumerate(learning_rate):
    pyplot.plot(n_estimators, -scores[i], label='learning_rate: ' + str(value))
pyplot.legend()
pyplot.xlabel('n_estimators')
pyplot.ylabel('Log Loss')
pyplot.savefig('n_estimators_vs_learning_rate2.png')

# testing for overfitting
train_scores, test_scores = list(), list()
list = [.1,.2,.3,.4,.5,.6,.7,.8,.9,1]
values = [i for i in list]
for i in values:
    xgb_model = xgb.XGBClassifier(use_label_encoder=False, objective="multi:softprob", random_state=42, eval_metric="merror", n_estimators=2800, learning_rate=0.01, max_depth=12, gamma=i)
    # train
    xgb_model.fit(x_train_encoded, y_train_encoded)
    train_yhat = xgb_model.predict(x_train_encoded)
    train_acc = accuracy_score(y_train_encoded, train_yhat)
    train_scores.append(train_acc)
    
    # validation
    test_yhat = xgb_model.predict(x_validation_encoded)
    test_acc = accuracy_score(y_validation_encoded, test_yhat)
    test_scores.append(test_acc)
    print('>%d, train: %.3f, test: %.3f' % (i, train_acc, test_acc))

pyplot.plot(values, train_scores, '-o', label='Train')
pyplot.plot(values, test_scores, '-o', label='Test')
pyplot.legend()
pyplot.show()
pyplot.savefig('trainVsTest001.png')

xgb_model = xgb.XGBClassifier(use_label_encoder=False, objective="multi:softprob", random_state=42, eval_metric="merror", n_estimators=2800, learning_rate=0.01, max_depth=12, gamma=0.3)
xgb_model.fit(x_train_encoded, y_train_encoded)

dtpklxgb = 'xgb_classifier.pkl'
dtxgbModelpkl = open(dtpklxgb, 'wb')
pickle.dump(xgb_model, dtxgbModelpkl)
dtxgbModelpkl.close()

# Classification Accuracy
yVald_pred_xgb = xgb_model.predict(x_validation_encoded)
yTrain_pred_xgb = xgb_model.predict(x_train_encoded)

print(accuracy_score(y_validation_encoded, yVald_pred_xgb))
print(confusion_matrix(y_validation_encoded, yVald_pred_xgb))

print(accuracy_score(y_train_encoded, yTrain_pred_xgb))
print(confusion_matrix(y_train_encoded, yTrain_pred_xgb))

#ROC curve graphs for each label
values2 = [0,1,2,3]
for i in values2:
    fpr, tpr, _ = roc_curve(y_train_encoded, yTrain_pred_xgb, pos_label=i)
    roc_auc = auc(fpr, tpr)
    #xgb.plot_importance(gbm)
    #plt.show()
    plt.figure()
    lw = 2
    plt.plot(fpr, tpr, color='darkorange',
             lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)
    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')
    plt.xlim([-0.02, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('ROC curve for label ' + str(i))
    plt.legend(loc="lower right")
    plt.show()

# importannce of each feature
xgb_model.feature_importances_

# 10 fold CV to check for overfitting
model = xgb.XGBClassifier(use_label_encoder=False, objective="multi:softprob", random_state=42, eval_metric="merror", n_estimators=2800, learning_rate=0.01, max_depth=12, gamma=0.3)
kfold = StratifiedKFold(n_splits=10, random_state=7)
results = cross_val_score(model, x_train_whole, y_train_whole, cv=kfold)
print("Accuracy: %.2f%% (%.2f%%)" % (results.mean()*100, results.std()*100))

"""**Decision Tree Classifier**"""

dtModel = DecisionTreeClassifier(random_state = 10)

dtModel.fit(x_train_encoded, y_train_encoded)

dtpklName = 'decisiontree_classifier.pkl'
dtModelpkl = open(dtpklName, 'wb')
pickle.dump(dtModel, dtModelpkl)
dtModelpkl.close()

dtModel.score(x_train_encoded, y_train_encoded)

dtModel.score(x_validation_encoded, y_validation_encoded)

yPred = dtModel.predict(x_validation_encoded)

"""**Visualize Decision Tree**"""

decisiontree = tree.export_graphviz(dtModel,out_file = 'tree.dot', feature_names = x_train_encoded.columns, max_depth=3, filled = True)

!dot -Tpng tree.dot -o tree.png

image = plt.imread('tree.png')
plt.figure(figsize=(15,15))
plt.imshow(image)

"""**Evaluating Decision Tree**

Adjusting Max Depth
"""

trainAccuracy = []
validationAccuracy = []
for depth in range(1,30):
  sampledtModel = DecisionTreeClassifier(max_depth = depth, random_state = 10)
  sampledtModel.fit(x_train_encoded, y_train_encoded)
  trainAccuracy.append(sampledtModel.score(x_train_encoded, y_train_encoded))
  validationAccuracy.append(sampledtModel.score(x_validation_encoded, y_validation_encoded))

df = pd.DataFrame({'maxDepth': range(1,30), 'trainingAccuracy': trainAccuracy, 'validationAccuracy': validationAccuracy})
df.head()

plt.figure(figsize=(12,6))
plt.plot(df['maxDepth'], df['trainingAccuracy'], '-o', label='Train')
plt.plot(df['maxDepth'], df['validationAccuracy'], '-o', label='Validation')
plt.xlabel('Depth of tree')
plt.ylabel('Performance')
plt.legend()
plt.savefig('maxDepthAdjust.png')

"""In the above graph we can see that the accuracy increases significantly to a max depth of 27 after which it evens out and slowly starts to decrease. Therefore using a max depth of 27 would yield the highest performance

Consfusion Matrix
"""

print(confusion_matrix(y_validation_encoded, yPred))
print(classification_report(y_validation_encoded, yPred))

accuracy_score(y_validation_encoded, yPred)

"""Feature importance"""

importance = pd.DataFrame({'feature': x_train_encoded.columns, 'importance' : np.round(dtModel.feature_importances_, 3)})
importance.sort_values('importance', ascending=False, inplace = True)
print(importance)

"""GridSearch CV Evaluation"""

tuned_parameters = [{'max_depth': [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25], 
                     'min_samples_split': [2,4,6,8,10]}]
scores = ['recall']
for score in scores:
    
    print()
    print(f"Tuning hyperparameters for {score}")
    print()
    
    dtModel = GridSearchCV(
        DecisionTreeClassifier(), tuned_parameters,
        scoring = f'{score}_macro'
    )
    dtModel.fit(x_train_encoded, y_train_encoded)
    
    print("Best parameters set found on development set:")
    print()
    print(dtModel.best_params_)
    print()
    print("Grid scores on development set:")
    means = dtModel.cv_results_["mean_test_score"]
    stds = dtModel.cv_results_["std_test_score"]
    for mean, std, params in zip(means, stds,
                                 dtModel.cv_results_['params']):
        print(f"{mean:0.3f} (+/-{std*2:0.03f}) for {params}")

"""**KNN Classifier**"""

knnModel = KNeighborsClassifier(n_neighbors = 17)

knnModel.fit(x_train_encoded, np.ravel(y_train_encoded))

knnpklName = 'knn_classifier.pkl'
knnModelpkl = open(knnpklName, 'wb')
pickle.dump(knnModel, knnModelpkl)
knnModelpkl.close()

knnModel.score(x_train_encoded, y_train_encoded)

knnModel.score(x_validation_encoded, y_validation_encoded)

yPredKnn_train = knnModel.predict(x_train_encoded)

yPredKnn = knnModel.predict(x_validation_encoded)

"""**Evaluating KNN**

Adjusting Nearest Neighbor
"""

trainAccuracy = []
validationAccuracy = []
for n_neigh in range(1, 31):
  sampleknnModel = KNeighborsClassifier(n_neighbors = n_neigh)
  sampleknnModel.fit(x_train_encoded, np.ravel(y_train_encoded))
  trainAccuracy.append(sampleknnModel.score(x_train_encoded, y_train_encoded))
  validationAccuracy.append(sampleknnModel.score(x_validation_encoded, y_validation_encoded))

df = pd.DataFrame({'n_neighbors': range(1,31), 'trainingAccuracy': trainAccuracy, 'validationAccuracy': validationAccuracy})
df.head()

plt.figure(figsize=(12,6))
plt.plot(df['n_neighbors'], df['trainingAccuracy'], '-o', label='Train')
plt.plot(df['n_neighbors'], df['validationAccuracy'], '-o', label='Validation')
plt.xlabel('Nearest Neighbors')
plt.ylabel('Performance')
plt.legend()

"""Confusion Matrix"""

print(confusion_matrix(y_train_encoded, yPredKnn_train))
print(classification_report(y_train_encoded, yPredKnn_train))

print("Accuracy Train {0:.2f}%".format(100*accuracy_score(y_train_encoded, yPredKnn_train)))

print(confusion_matrix(y_validation_encoded, yPredKnn))
print(classification_report(y_validation_encoded, yPredKnn))

print("Accuracy Validation {0:.2f}%".format(100*accuracy_score(y_validation_encoded, yPredKnn)))